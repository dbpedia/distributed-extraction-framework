# The SPARK_HOME environment variable should be set to this, Spark's location
spark-home=/home/gonephishing/spark-1.3.0/

# Paths to the Hadoop configuration files, if any. These are needed for HDFS.
# hadoop-coresite-xml-path=/home/user/engine/hadoop-2.2.0/etc/hadoop/core-site.xml
# hadoop-hdfssite-xml-path=/home/user/engine/hadoop-2.2.0/etc/hadoop/hdfs-site.xml
# hadoop-mapredsite-xml-path=/home/user/engine/hadoop-2.2.0/etc/hadoop/mapred-site.xml

# Refer to README.md for advice
spark.executor.memory=2500m

# Replace local[8] with something like spark://192.168.0.100 to go into distributed mode.
spark-master=local[4]

# When running on a distributed cluster, it is essential that you set spark.cores.max to N * M
# where N = total no. of slave machines, M = SPARK_WORKER_INSTANCES (from spark-env.sh)
# This is to ensure that Spark uses as many cores (over the entire cluster) as many workers there are.
spark.cores.max=4

# You can add more spark.* variables here. All variables starting with spark. will be provided to SparkConf

# This is used for setting log levels for "org.apache", "spark", "org.eclipse.jetty" and "akka" using
# SparkUtils.setLogLevels(). It is WARN by default to prevent excessive logging from Spark.
# It is a good idea to set it to INFO while debugging/testing out the framework.
# Refer to org.apache.log4j.Level for more details
# logging-level=INFO

# WARNING: If base-dir is set here, the base-dir in config.properties (the original DBpedia extraction configuration) is ignored.
# base-dir=/data

# Please refer to the source code for org.dbpedia.extraction.dump.extract.DistConfig for the complete set of configuration variables
# TODO: Add info on all configuration variables here.