# NOTE: format is not java.util.Properties, but org.dbpedia.extraction.dump.download.DownloadConfig

#distconfig=/home/gonephishing/dbpedia-extraction/distributed-extraction-framework/download/src/test/resources/dist-download.properties
#   Path to existing distributed download configuration text file (UTF-8) whose lines contain arguments
#   in the format given here. Absolute or relative path. File paths in that config file will be interpreted
#   relative to the config file.

extraction-framework-home=/home/gonephishing/dbpedia-extraction/distributed-extraction-framework/
#   This must be set to the absolute path to the distributed extraction framework (containing this module)
#   in all nodes. No default value is set.

mirrors=http://dumps.wikimedia.org/,http://wikipedia.c3sl.ufpr.br/,http://ftp.fi.muni.cz/pub/wikimedia/,http://dumps.wikimedia.your.org/
#  List of mirrors to download from in the form of comma-separated URLs. Choose from the list of mirrors at:
#  http://meta.wikimedia.org/wiki/Mirroring_Wikimedia_project_XML_dumps#Current_Mirrors
#  Example: mirrors=http://dumps.wikimedia.org/,http://wikipedia.c3sl.ufpr.br,http://ftp.fi.muni.cz/pub/wikimedia/,http://dumps.wikimedia.your.org/

threads-per-mirror=2
#  Number of simultaneous downloads from each mirror per slave node. Set to 2 by default.

workers-per-slave=2
#  Number of workers to run per slave. This is set to 2 by default.
#  Setting it to (no. of mirrors) * threads-per-mirror is recommended for exploiting maximum parallelism. On the other hand,
#  if your whole cluster has only one public facing IP it is better to set this to a low number like 1.

progress-interval=2
#  Progress report time interval in secs - the driver node receives real-time progress reports for running downloads from the workers.
#  If a worker fails to send a progress report of the current download under the given timeout (the timeout is set to something
#  like progressReportInterval + 2 to be safe) the download job will be marked as failed and inserted back into the pending
#  download queue. This is 2 seconds by default.

max-duplicate-progress-reports=30
#  Maximum number of consecutive duplicate progress read bytes to tolerate. The workers keep track of download progress;
#  if a download gets stuck consecutive progress reports will contain the same number of bytes downloaded. If this is set
#  to 30 (not recommended to go below that), the worker will declare a job as failed only after getting the same progress
#  report for 30 times. By default set to 30.

local-temp-dir=data/
#  Local temporary directory on worker nodes. Each dump file/chunk is downloaded to this directory before being moved to
#  the configured Hadoop file system. This is /tmp by default.

private-key=/home/gonephishing/.ssh/id_rsa
#  Optional identity file to connect to cluster nodes via SSH.

#ssh-passphrase=passphrase
#  Optional passphrase for SSH private key.

sequential-languages=false
#  If each language consists of multiple dump files (eg. enwiki-latest-pages-articles1.xml-p000000010p000010000.bz2)
#  they are downloaded in parallel. Multiple languages are downloaded in parallel too, giving us 2 levels of
#  parallelism. If sequentialLanguages is set to true, one language is downloaded at a time, otherwise,
#  all languages are downloaded in parallel.

#hadoop-coresite-xml-path=/path/to/core-site.xml
#  Path to hadoop core-site.xml configuration file.

#hadoop-hdfssite-xml-path=/path/to/hdfs-site.xml
#  Path to hadoop hdfs-site.xml configuration file.

#hadoop-mapredsite-xml-path=/path/to/mapred-site.xml
#  Path to hadoop mapred-site.xml configuration file.

master=127.0.0.1
#  Master node host.

slaves=127.0.0.1
#  List of comma-separated slave hosts. Example: slaves=node1,node2,node3

base-dir=/home/gonephishing/dbpedia-extraction/distributed-extraction-framework/data
#  Replace by your target folder. If this is omitted here, it is read from the general configuration file if there is any.

#join=akka.tcp://Workers@hostname:port
#  This variable needs to be specified when starting up a worker manually. Do not use this variable unless you know what you're
#  doing. The driver node automatically starts up workers on the slaves and takes care of this variable. Never set this variable
#  when starting up the master/driver.